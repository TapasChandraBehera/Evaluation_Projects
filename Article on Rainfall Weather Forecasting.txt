Rainfall weather forecasting.

Importing required libraries.
import pandas as pd

#Loading the dataset
df=pd.read_csv('rainfall_forecasting.csv')
df

Exploratory Data Analysis(EDA).
#To show first few records
df.head()

#To show last few records
df.tail()

#checking statistical summary
df.describe()

#checking datatypes for each columns
df.dtypes

#Checking for null values
df.isnull().sum()

Since I'm facing some missing values in my dataset. Let's first handle this nan values.
# Impute missing values for numeric columns
numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am',
                'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm',
                'Temp9am', 'Temp3pm']
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

# Impute missing values for categorical columns
categorical_cols = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']
df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])
df.isnull().sum()

# Statistical Summary
numeric_summary = df.describe()
print(numeric_summary)

# Distribution of Target Variable
target_distribution = df['RainTomorrow'].value_counts()
print(target_distribution)
output:-
No     6434
Yes    1991
Name: RainTomorrow, dtype: int64

# Visualize the distribution
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
target_distribution.plot(kind='bar', color=['skyblue', 'salmon'])
plt.xlabel('Rain Tomorrow')
plt.ylabel('Count')
plt.title('Distribution of Rain Tomorrow')
plt.xticks(rotation=0)
plt.show()

# Scatter Plot - Relationship between Rainfall and RainTomorrow
plt.figure(figsize=(8, 6))
plt.scatter(df['Rainfall'], df['RainTomorrow'], alpha=0.1)
plt.xlabel('Rainfall (mm)')
plt.ylabel('Rain Tomorrow')
plt.title('Relationship between Rainfall and Rain Tomorrow')
plt.show()

# Scatter Plot - Relationship between MaxTemp and RainTomorrow
plt.figure(figsize=(8, 6))
plt.scatter(df['MaxTemp'], df['RainTomorrow'], alpha=0.1)
plt.xlabel('Max Temperature (°C)')
plt.ylabel('Rain Tomorrow')
plt.title('Relationship between Max Temperature and Rain Tomorrow')
plt.show()

# Scatter Plot - Relationship between Rainfall and RainTomorrow
plt.figure(figsize=(8, 6))
plt.scatter(df['Rainfall'], df['RainTomorrow'], alpha=0.1)
plt.xlabel('Rainfall (mm)')
plt.ylabel('Rain Tomorrow')
plt.title('Relationship between Rainfall and Rain Tomorrow')
plt.show()

# Scatter Plot - Relationship between MaxTemp and RainTomorrow
plt.figure(figsize=(8, 6))
plt.scatter(df['MaxTemp'], df['RainTomorrow'], alpha=0.1)
plt.xlabel('Max Temperature (°C)')
plt.ylabel('Rain Tomorrow')
plt.title('Relationship between Max Temperature and Rain Tomorrow')
plt.show()

Feature selection, Model Building and Model Evaluation.
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Encode categorical columns
label_encoder = LabelEncoder()
categorical_cols = ['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']
df[categorical_cols] = df[categorical_cols].apply(label_encoder.fit_transform)

# Splitting the data into training and testing sets
X = df.drop(['RainTomorrow', 'Date', 'Location'], axis=1)  # Features
y = df['RainTomorrow']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Building - Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

output:-
RandomForestClassifier(random_state=42)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

# Model Evaluation
y_pred = rf_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
output:-
Accuracy: 0.9157270029673591

Confusion Matrix:
 [[1276   38]
 [ 104  267]]

Classification Report:
               precision    recall  f1-score   support

           0       0.92      0.97      0.95      1314
           1       0.88      0.72      0.79       371

    accuracy                           0.92      1685
   macro avg       0.90      0.85      0.87      1685
weighted avg       0.91      0.92      0.91      1685


Hyperparameter Tuning.
from sklearn.model_selection import GridSearchCV
# Define the hyperparameters grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)
output:-
GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,
             param_grid={'max_depth': [None, 5, 10, 15],
                         'min_samples_leaf': [1, 2, 4],
                         'min_samples_split': [2, 5, 10],
                         'n_estimators': [100, 200, 300]},
             scoring='accuracy')
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

# Get the best hyperparameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

print("Best Hyperparameters:")
print(best_params)
output:-
Best Hyperparameters:
{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}


Saving the best model using pickle.
import pickle
with open('best_rf_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)